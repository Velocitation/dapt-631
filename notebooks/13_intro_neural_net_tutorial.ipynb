{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Neural Network (Digits Recognition) Model [Tutorial]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image data\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# modeling\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# data processing\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = load_digits()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1347, 64) (1347, 10) (450, 64) (450, 10)\n"
     ]
    }
   ],
   "source": [
    "# grab all data (1797 records, and 8x8=64 columns)\n",
    "X = digits.data\n",
    "\n",
    "# grab the target (true) value for each image\n",
    "y = digits.target\n",
    "\n",
    "X_train, X_test, y_train_raw, y_test_raw = train_test_split(X, y, random_state=314)\n",
    "\n",
    "y_train = pd.get_dummies(y_train_raw).values\n",
    "y_test = pd.get_dummies(y_test_raw).values\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start, we need to take a small digression. The keras Neural Network model training results are not easily reproducible since it involves a lot of shuffling and random initializations. In order to maintain consitency, we will have to initialize some random seeds before every model run. We will create a function to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import random as tf_random\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def init_seeds(s):\n",
    "    '''\n",
    "    Initializes random seeds prior to model training \n",
    "    to ensure reproducibality of training results.\n",
    "    '''\n",
    "    tf_random.set_seed(s)\n",
    "    np.random.seed(s)\n",
    "    random.seed(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-layer Perceptron (MLP) model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a model with one input layer and one output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "\n",
    "mlp1 = Sequential(\n",
    "    [\n",
    "        Input(shape=(64,)),\n",
    "        Dense(10, activation='softmax')\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m650\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> (2.54 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m650\u001b[0m (2.54 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> (2.54 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m650\u001b[0m (2.54 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mlp1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have initialized a sequential model with an input shape of 64, and an output layer with a shape of 10. There are 650 total parameters in this model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial weights and biases\n",
    "#--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**Useful Resources:**_\n",
    "\n",
    "* [Keras Documentation: The Sequential model](https://keras.io/guides/sequential_model/)\n",
    "* [TensorFlow API Documentation: tf.keras.Sequential](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can train the model, we need to specify training parameters (aka compile the model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp1.compile(optimizer='sgd', loss='categorical_crossentropy',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train (fit) the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 999us/step - accuracy: 0.3024 - loss: 6.1153  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x193b113c1a0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp1.fit(X_train, y_train, shuffle=False) #should be set to true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is very low, because the model made a single pass over the dataset. The model must make multiple passes over the entire dataset in order to produce better results. This can be adjusted by using the `epoch` parameter.\n",
    "\n",
    "Epoch is equal to the number of times the algorithm sees the entire dataset.\n",
    "\n",
    "By the way, what is that mysterious-looking number 43? Where did that come from?\n",
    "\n",
    "Keras uses a `batch size` of 32 by default. So this model above created 32 batches of 43 records each by divinding the total number of records in the entire dataset (1,347) by 32. However, the default `epoch` is 1, so the model above made 43 **iterations** (forward + backward) using batches of 52 records.\n",
    "\n",
    "`batch_size * number of iterations` --> `epoch`\n",
    "\n",
    "Note that if you use `shuffle=True` in the `fit()` function, keras will shuffle the records in the training dataset before splitting them into batches.\n",
    "\n",
    "Let's increase the number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 735us/step - accuracy: 0.3206 - loss: 6.2504  \n",
      "Epoch 2/7\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 633us/step - accuracy: 0.7463 - loss: 1.2794\n",
      "Epoch 3/7\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 644us/step - accuracy: 0.8264 - loss: 0.6766\n",
      "Epoch 4/7\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 667us/step - accuracy: 0.9152 - loss: 0.3117\n",
      "Epoch 5/7\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 629us/step - accuracy: 0.9299 - loss: 0.2604\n",
      "Epoch 6/7\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 724us/step - accuracy: 0.9378 - loss: 0.2259\n",
      "Epoch 7/7\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 671us/step - accuracy: 0.9435 - loss: 0.1999\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x193b1155ac0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize seeds\n",
    "init_seeds(314)\n",
    "\n",
    "# prepare the model architecture\n",
    "mlp2 = Sequential(\n",
    "    [\n",
    "        Input(shape=(64,)),\n",
    "        Dense(10, activation='softmax', name='output_layer')\n",
    "    ],\n",
    "    name='simple_mlp_7_epochs')\n",
    "mlp2.compile(optimizer='sgd', loss='categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "mlp2.fit(X_train, y_train, epochs=7, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that now the model is making 7 passes through the entire data. For each epoch, there are 43 iterations (forward + backward) with 32 samples in each iteration.\n",
    "\n",
    "Let's try to reduce the batch size and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "\u001b[1m270/270\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 565us/step - accuracy: 0.5935 - loss: 3.6357   \n",
      "Epoch 2/7\n",
      "\u001b[1m270/270\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 606us/step - accuracy: 0.9097 - loss: 0.3841\n",
      "Epoch 3/7\n",
      "\u001b[1m270/270\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 592us/step - accuracy: 0.9492 - loss: 0.2488\n",
      "Epoch 4/7\n",
      "\u001b[1m270/270\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 590us/step - accuracy: 0.9513 - loss: 0.1799\n",
      "Epoch 5/7\n",
      "\u001b[1m270/270\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 598us/step - accuracy: 0.9712 - loss: 0.1412\n",
      "Epoch 6/7\n",
      "\u001b[1m270/270\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 654us/step - accuracy: 0.9727 - loss: 0.1111\n",
      "Epoch 7/7\n",
      "\u001b[1m270/270\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 591us/step - accuracy: 0.9745 - loss: 0.0987\n"
     ]
    }
   ],
   "source": [
    "# initialize seeds\n",
    "init_seeds(314)\n",
    "\n",
    "# prepare the model architecture\n",
    "mlp3 = Sequential(\n",
    "    [\n",
    "        Input(shape=(64,)),\n",
    "        Dense(10, activation='softmax', name='output_layer')\n",
    "    ], \n",
    "    name='simple_mlp_7epochs_5bs')\n",
    "\n",
    "mlp3.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "mlp3.fit(X_train, y_train, batch_size=5, epochs=7, shuffle=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The batch size of 5 gives us 270 iterations for each epoch.\n",
    "\n",
    "**Useful Resources:**\n",
    "* [Keras FAQ: What do sample, batch, and epoch mean?](https://keras.io/getting_started/faq/#what-do-sample-batch-and-epoch-mean)\n",
    "* [What is the trade-off between batch size and number of iterations to train a neural network?](https://stats.stackexchange.com/questions/164876/what-is-the-trade-off-between-batch-size-and-number-of-iterations-to-train-a-neu)\n",
    "\n",
    "Tip: Using larger batch sizes will require more memory, and negatively impact the ability of the model to generalize well. On the other hand, very small batch size increases the risk of making the model unreliable (too stochastic.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the model accuracy on the test sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 784us/step - accuracy: 0.6643 - loss: 2.4530\n",
      "Loss: 265.75%, Accuracy: 63.11%\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = mlp1.evaluate(X_test, y_test)\n",
    "\n",
    "print(f'Loss: {loss:.2%}, Accuracy: {accuracy:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use this multiple times, so let's create a small function for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 505us/step - accuracy: 0.6778 - loss: 2.3438\n",
      "Loss: 280.43%, Accuracy: 63.11%\n"
     ]
    }
   ],
   "source": [
    "def test_model(model, test_data, lables):\n",
    "    loss, accuracy = model.evaluate(test_data, lables, batch_size=1)\n",
    "    print(f'Loss: {loss:.2%}, Accuracy: {accuracy:.2%}')\n",
    "    \n",
    "test_model(mlp1, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model `mlp3` with a batch size of five and seven epochs seems to be performing the best.\n",
    "\n",
    "Next, let's add one hidden layer to this model and see how much improvement it yields."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add a hidden layer to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "270/270 - 0s - 2ms/step - accuracy: 0.1203 - loss: 2.2696\n",
      "Epoch 2/15\n",
      "270/270 - 0s - 698us/step - accuracy: 0.2754 - loss: 2.1430\n",
      "Epoch 3/15\n",
      "270/270 - 0s - 688us/step - accuracy: 0.3185 - loss: 2.0522\n",
      "Epoch 4/15\n",
      "270/270 - 0s - 718us/step - accuracy: 0.3422 - loss: 1.9636\n",
      "Epoch 5/15\n",
      "270/270 - 0s - 839us/step - accuracy: 0.3586 - loss: 1.8837\n",
      "Epoch 6/15\n",
      "270/270 - 0s - 720us/step - accuracy: 0.3801 - loss: 1.8088\n",
      "Epoch 7/15\n",
      "270/270 - 0s - 739us/step - accuracy: 0.3898 - loss: 1.7433\n",
      "Epoch 8/15\n",
      "270/270 - 0s - 717us/step - accuracy: 0.3808 - loss: 1.6864\n",
      "Epoch 9/15\n",
      "270/270 - 0s - 694us/step - accuracy: 0.3831 - loss: 1.6322\n",
      "Epoch 10/15\n",
      "270/270 - 0s - 736us/step - accuracy: 0.3927 - loss: 1.5732\n",
      "Epoch 11/15\n",
      "270/270 - 0s - 717us/step - accuracy: 0.3890 - loss: 1.5218\n",
      "Epoch 12/15\n",
      "270/270 - 0s - 776us/step - accuracy: 0.3920 - loss: 1.4711\n",
      "Epoch 13/15\n",
      "270/270 - 0s - 720us/step - accuracy: 0.3883 - loss: 1.4279\n",
      "Epoch 14/15\n",
      "270/270 - 0s - 698us/step - accuracy: 0.3890 - loss: 1.3926\n",
      "Epoch 15/15\n",
      "270/270 - 0s - 769us/step - accuracy: 0.3868 - loss: 1.3614\n"
     ]
    }
   ],
   "source": [
    "# initialize seeds\n",
    "init_seeds(314)\n",
    "\n",
    "# prepare the model architecture\n",
    "mlp4 = Sequential(\n",
    "    [\n",
    "    Input(shape=(64,)),\n",
    "    Dense(32, activation='softmax', name='hidden_layer'),\n",
    "    Dense(10, activation='softmax', name='output_layer')\n",
    "    ],\n",
    "    name='simple_mlp_7_epochs'\n",
    ")\n",
    "\n",
    "mlp4.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "mlp4.fit(X_train, y_train, batch_size=5, epochs=15, shuffle=True, verbose=2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 636us/step - accuracy: 0.3862 - loss: 1.3903\n",
      "Loss: 141.50%, Accuracy: 39.33%\n"
     ]
    }
   ],
   "source": [
    "test_model(mlp4, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding one hidden layer, and increasing the number of epochs, improved the model's accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interesting questions:**\n",
    "\n",
    "* [How to choose the number of hidden layers and the size of hidden layers?](https://stackoverflow.com/questions/10565868/multi-layer-perceptron-mlp-architecture-criteria-for-choosing-number-of-hidde?lq=1)\n",
    "* [Why are neural networks becoming deeper, but not wider?](https://stats.stackexchange.com/questions/222883/why-are-neural-networks-becoming-deeper-but-not-wider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try a different activation function (relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 565us/step - accuracy: 0.3862 - loss: 1.3903\n",
      "Loss: 141.50%, Accuracy: 39.33%\n"
     ]
    }
   ],
   "source": [
    "# initialize seeds\n",
    "init_seeds(314)\n",
    "\n",
    "# prepare the model architecture\n",
    "mlp5 = Sequential(\n",
    "    [\n",
    "        Input(shape=(64,)),\n",
    "        Dense(32, activation='softmax', name='hidden_layer'),\n",
    "        Dense(10, activation='softmax', name='output_layer')\n",
    "    ],\n",
    "    name='mlp_1hidden_relu')\n",
    "\n",
    "mlp5.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "mlp5.fit(X_train, y_train, batch_size=5, epochs=15, shuffle=True, verbose=0)\n",
    "\n",
    "test_model(mlp5, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing the activation function did not improve the model performance for this dataset. However, the `relu` activation function usually outperforms `sigmoid` and it's usually preferred."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try a different optimizer (adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "expected argument value expression (4184832528.py, line 13)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[45], line 13\u001b[1;36m\u001b[0m\n\u001b[1;33m    mlp6.compile(optimizer=, loss='categorical_crossentropy', metrics='accuracy')\u001b[0m\n\u001b[1;37m                 ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m expected argument value expression\n"
     ]
    }
   ],
   "source": [
    "# initialize seeds\n",
    "init_seeds(314)\n",
    "\n",
    "# prepare the model architecture\n",
    "mlp6 = Sequential(\n",
    "    [\n",
    "        Input(shape=8*8),\n",
    "        Dense(32, activation='relu', name='hidden_layer'),\n",
    "        Dense(10, activation='softmax', name='output_layer')\n",
    "    ],\n",
    "    name='mlp_1hidden_adam')\n",
    "\n",
    "mlp6.compile(optimizer=, loss='categorical_crossentropy', metrics='accuracy')\n",
    "\n",
    "mlp6.fit(X_train, y_train, batch_size=5, epochs=15, shuffle=True, verbose=0)\n",
    "\n",
    "test_model(mlp6, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The adam optimizer improved the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Useful resource:**\n",
    "\n",
    "* [Keras Documentation: Adam](https://keras.io/api/optimizers/adam/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate predicted probabilities\n",
    "probs = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can use formatted string literal to print float values instead\n",
    "[f'{x:.5f}' for x in probs[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model returns a probability for each lable (digit). We can grab the lable (digit) that has the highest probability. For example, for the first sample, the model is predicting a very high probability of 0.99974 for that digit to be 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab the digit with max probability\n",
    "y_preds = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfm = \n",
    "plt.figure(figsize=(6, 6))\n",
    "\n",
    "sns.heatmap()\n",
    "plt.xlabel('Predicted value', fontsize=12)\n",
    "plt.ylabel('True value', fontsize=12)\n",
    "plt.title('Confusion Matrix (Neural Network)', fontsize=12, weight='semibold');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving (exporting) the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model as a single `HDF5` file. [`HDF5` stands for Hierarchical Data Format, v5.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_path = '../misc/digits_recognition_mlp_model.h5'\n",
    "\n",
    "#--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This saves the trained model and all trackable objects (config, weights, and optimizer) attached to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a saved model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can load a pre-trained (and saved) model by using `load_model()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the model that we just loaded to predict values\n",
    "#--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Useful resources:**\n",
    "* [Keras: Serialization and saving](https://keras.io/guides/serialization_and_saving/)\n",
    "* [Keras: Model saving and serialization APIs](https://keras.io/api/models/model_saving_apis/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab raw model predictions for the entire dataset\n",
    "preds_raw = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab the predicted digit (based on the highest probability) for the entire dataset\n",
    "preds = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the first 100 predictions. The labels are shown in green if the model predicted the digit correcly, otherwise it's shown in red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(10, 10, figsize=(7, 7), subplot_kw={'xticks':[], 'yticks':[]})\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(digits.images[i], cmap='binary', interpolation='bicubic')\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    pred = preds[i]\n",
    "    act = y[i]\n",
    "    if pred == act:\n",
    "        ax.text(0.05, 0.05, preds[i], color='green',\n",
    "                weight='semibold', transform=ax.transAxes)\n",
    "    else:\n",
    "        ax.text(0.05, 0.05, preds[i], color='tomato',\n",
    "                weight='semibold', transform=ax.transAxes)\n",
    "plt.show();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
